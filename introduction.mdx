---
title: What is Aeglos?
---

<span>
Aeglos essentially ensures that your LLM agents are constantly being monitered to ensure
consistent security and reliability. This means that there is no more scope for both external malicious inputs and
internal attackable prompts to mess up your results.

Right now we primarily support langchain (python only, js coming soon!), allowing you to guard your <a href="https://python.langchain.com/docs/modules/agents/">Langchain agents</a>
and <a href="https://python.langchain.com/docs/modules/chains">LCEL chains</a>.

</span>

<img
  className="block dark:hidden"
  src="/images/hero-light.svg"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="/images/hero-dark.svg"
  alt="Hero Dark"
/>

## What we prevent (to name a few)

<CardGroup cols={2}>
  <Card title="Indirect Prompt Injection" icon="syringe">
    <span>
      Malicious inputs hidden in external resources can no longer mess up your
      system
    </span>
  </Card>

  <Card title="Code Injection" icon="code">
    <span>
      Malicious code entered in no longer has the potential to disrupt anything
    </span>

  </Card>

  <Card
    title="Context Leakage"
    icon="code"
   
  >
  Prevent situations where LLM's inadvertently disclose confidential context/information
  </Card>
   <Card
    title="Virtualization"
    icon="vr-cardboard"
  >
  Prompts can no longer "set the scene" for agents, putting them in a position to 
  divulge incorrect information/ act unethically
  </Card>
</CardGroup>
